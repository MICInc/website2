<template>
	<div>
		<h2>{{ speakers[person].name }}</h2>
		<img v-if="speakers[person].headshot != null" :src="'/img/speakers/'+speakers[person].headshot" alt="speaker headshot">
		<div>
			<h3>Bio:</h3>
			<p v-html="speakers[person].bio"></p>
			<span v-if="speakers[person].title != null">
				<h3>Talk:</h3>
				<h3>{{ speakers[person].title }}</h3>
				<p v-if="speakers[person].abstract != null" v-html="speakers[person].abstract"></p>
			</span>
		</div>
	</div>
</template>

<script>
export default {
	name: 'speakers2019',
	beforeMount() {
		this.person = this.$route.params.id;
	},
	data() {
		return {
			speakers: {
				anelise_newman: {
					name: 'Anelise Newman',
					bio: 'Anelise is a Master of Engineering student working with Aude Oliva in the Computational Perception and Cognition group in CSAIL. Her work deals with bringing together computer vision, human cognition, and HCI to build systems that predict human responses to visual stimuli. In addition to studying video memorability, her recent work focuses on predicting where people will look on an image and developing crowdsourceable UIs to collect attention data at scale.',
					title: 'Memento: Modeling Video Memorability',
					abstract: 'Why do you remember your favorite childhood memory, but can’t recall what you ate for breakfast yesterday? Why can you remember certain scenes from a movie while others fade into oblivion? When you take your own videos for social media, can you tell which ones will stick most in the minds of your audience? In other words, what makes a video memorable, and can you predict it? There is a lot of previous work on understanding image memorability, but videos are less studied and pose additional challenges. The goal of the Memento Project is to understand what makes a standout video and then predict—perhaps even before a human has seen it—how memorable it will be. In this talk I will describe how we collected objective memorability scores for over ten thousand images using an online memorability game and how analyzing this data led to new insights about human memory. I’ll talk about how we used it to train a state-of-the-art video memorability model that makes predictions based on both appearance and video dynamics. I’ll also explain the future of the Memento Project and how we’re planning to reach our goal of collecting memorability scores for 1 million videos. Understanding video memorability at a large scale is key not just for making compelling Snapchat stories, but also for bridging the gap between static images and the real world.',
					headshot: 'anelise_newman.jpg'
				},
				anish_agarwal: {
					name: 'Anish Agarwal',
					bio: 'Anish Agarwal is a 3rd PhD student in EECS at MIT co-advised by Munther Dahleh and Devavrat Shah. He received his BS and MS degree in Computer Science from Caltech. Before joining MIT, he was a management consultant at Boston Consulting Group (BCG), and was the head of Machine Learning in a Bay Area startup, Ikasi Inc. His research interests are in high-dimensional statistics and the design of data marketplaces.',
					title: 'Zorro - Preserve your privacy and get paid for it',
					abstract: 'As the main contribution, we provide a natural, \"absolute\" definition of \"Value of Data\" (VoD) – for any quantity of interest, it is the delta between an individual\'s-value and population-mean. The challenge remains how to operationalize this definition, independently of a buyer\'s model for VoD. We propose a model-agnostic solution, relying on matrix-estimation, and show how to use it to estimate click-through-rate (CTR), for example.<br><br>Regarding (2), Zorro empowers advertisers to measure value of user-data: (i) on a query-by-query basis; (ii) based only on increase-in-accuracy it provides in estimating CTR. This is in contrast with inefficient long-term-contracts advertisers are engaged in currently with third-party-data-sellers. We highlight two experimental results on a large real-world ad-click dataset. (i) Our CTR estimation system has R2=0.58, in line with state-of-the-art results for comparable problems (e.g. content-recommendation). Crucially, our system is model-agnostic, i.e., we estimate CTR without accessing an advertiser\'s proprietary models, a necessary property of any such pricing-system. (ii) With respect to our definition of VoD, experiments show selling user-data has incremental value ranging from 30%-69% depending on advertiser-category. Roughly, this translates to at least USD 16-Billion loss in value for advertisers if user-data is not provided.<br><br>Regarding (1), in addition to allowing users to get paid for data-sharing, we extend our mathematical framework to when users provide explicit intent for types of ads they want to see.',
					headshot: 'anish_agarwal.jpg'
				},
				aude_oliva: {
					name: 'Aude Oliva',
					bio: 'Aude Oliva is the executive director of MIT Quest for Intelligence and the MIT–IBM Watson AI Lab. She is also a principal research scientist at the Computer Science and Artificial Intelligence Laboratory. She formerly served as an expert to the National Science Foundation, Directorate of Computer and Information Science and Engineering. Her research interests span computer vision, cognitive science and human neuroscience. She was honored with the National Science Foundation CAREER Award, a Guggenheim Fellowship and the Vannevar Bush Faculty Fellowship. She earned a MS and PhD in cognitive science from the Institut National Polytechnique de Grenoble, France.',
					title: 'Closing keynote: Natural Deep Learning Networks',
					headshot: 'aude_oliva.jpg'
				},
				david_cox: {
					name: 'David Cox',
					bio: 'David Cox is the IBM Director of the MIT-IBM Watson AI Lab, a first of its kind industry-academic collaboration between IBM and MIT, focused on fundamental research in artificial intelligence. The Lab was founded with a $240m, 10 year commitment from IBM and brings together researchers at IBM with faculty at MIT to tackle hard problems at the vanguard of AI.<br><br>Prior to joining IBM, David was the John L. Loeb Associate Professor of the Natural Sciences and of Engineering and Applied Sciences at Harvard University, where he held appointments in Computer Science, the Department of Molecular and Cellular Biology and the Center for Brain Science. David\'s ongoing research is primarily focused on bringing insights from neuroscience into machine learning and computer vision research. His work has spanned a variety of disciplines, from imaging and electrophysiology experiments in living brains, to the development of machine learning and computer vision methods, to applied machine learning and high performance computing methods.<br><br>David is a Faculty Associate at the Berkman-Klein Center for Internet and Society at Harvard Law School and is an Agenda Contributor at the World Economic Forum. He has received a variety of honors, including the Richard and Susan Smith Foundation Award for Excellence in Biomedical Research, the Google Faculty Research Award in Computer Science, and the Roslyn Abramson Award for Excellence in Undergraduate Teaching. He led the development of \"The Fundamentals of Neuroscience\" (<a=\"http://fundamentalsofneuroscience.org\">http://fundamentalsofneuroscience.org</a>) one of Harvard\'s first massive open online courses, which has drawn over 750,000 students from around the world.  His academic lab has spawned several startups across a range of industries, ranging from AI for healthcare to autonomous vehicles.',
					title: 'IBM Technical Talk',
					abstract: 'TBA',
					headshot: 'david_cox.jpg'
				},
				dat_huynh: {
					name: 'Dat Huynh',
					bio: 'Dat Huynh is a Ph.D. student in the Computer Science program at Northeastern University’s  Khoury College of Computer Sciences, advised by Professor Ehsan Elhamifar. His research is dedicated to transfer learning which is the task of leveraging information from one domain to perform tasks in different (possibly related) domains.',
					title: 'Learning with limited data',
					abstract: 'Recent advances in machine learning, especially deep learning, have reached super-human performance such as on image classification (measured on ImageNet dataset). These learning techniques require an enormous amount of training data (thousand of training sample). However, humans only require a few sample images to learn new concepts. Remarkably, we can also recognize unseen objects based on their textual descriptions. In this talk, I will discuss different techniques for learning without a large amount of human supervision in order to bridge the gap between human and machine learning.<br><br>Ultimately, machine intelligence can only be achieved if the machine can efficiently learn without any human supervision (annotated data) since the machine will have to reason by itself as opposed to mimic human decision. Reducing the amount of training data is a step toward that goal.',
					headshot: 'dat_huynh.jpg'
				},
				efe_akengin: {
					name: 'Mehmet Efe Akengin',
					bio: 'Mehmet Efe Akengin has graduated from MIT with double-major in Computer Science and Political Science. Efe is working at a stealth-mode startup ecosystem that internally builds and launches high-tech ventures. Before that, he worked on a Data Science and Machine Learning Consultancy called Q Labs to help Fortune 500 companies with their data transformation processes. He is the co-founder and executive chairman of a Turkish AI startup called Tria AI that builds state-of-the-art ML solutions for the most impactful businesses in the region. He started these two companies after seeing the transformative potential of using big data in traditional industries when he was doing an Innovation Fellowship with the CEO of Energy From Waste, where he has created the first data strategy for Europe\'s largest waste to energy company.\n\nAt MIT, he was the co-founder of the MIT Machine Intelligence Community, and founder of Nav Talent\'s MIT branch that connects talented engineers with high-growth startups.',
					title: 'Machine Intelligence in Research and Industry',
					abstract: 'This breakout will be a discussion-based session covering some of the key differences between machine intelligence research in academia and industry environments, as well as how progress in each can drive and influence innovation in the other. Our goal is to foster a community-sourced conversation about the interplay of research and product development. Especially as companies across the globe increase efforts to integrate artificial intelligence techniques into their day-to-day operations, we feel it is especially important to be cognizant of the societal and economic motivations that influence what research is done, and how quickly it is adopted into industry. We encourage participants to share anecdotal experiences as well as any aspirations or questions regarding future career paths.',
					headshot: 'efe_akengin.png'
				},
				hassan_kane: {
					name: 'Hassan Kané',
					bio: 'I am a technology entrepreneur and AI researcher interested in using the same level of ambition, innovation and drive found in places like Silicon Valley and MIT to create a sustainable future in which humanity lives in harmony/balance with our environment.<br><br>Practically, this goal has translated into leading technology companies focused on social impact, building communities and writing. I am currently CTO at Sela Technologies, Board member at Investiv Group.<br><br>I was born and raised in Côte d\'Ivoire, moved to the US at the age of 17 and subsequently attended MIT where I studied Computer Science. At MIT, I was a co-founder of the Machine Intelligence Community which started as a paper reading group in Spring 2016.',
					title: 'AI & Climate Change: What role can machine learning students, engineers and scientists play in leading the transition toward a sustainable future?',
					abstract: 'In the beginning of the 21st century, the ML community has gathered some of the world’s most ambitious scientists, engineers and policy makers around a central language: That of data, prediction and modeling. This diverse community has a considerable amount of attention, capital, talent blended with a unique culture of openness, collaboration, which makes it a leader among scientific communities.<br><br>Many in the ML community wish to take action on climate change, yet feel their skills are inapplicable.  In this breakout session, we will discuss tackle two important topics:<ul><li>What role does the machine learning community currently play in consuming natural resources and emitting greenhouse gases (i.e model training, conferences) and how can we mitigate it?</li><li>What perspective can the community bring to reduce greenhouse gas emissions and in helping society adapt to the effects of climate change?</li></ul><br><b>Recommended readings:</b><ul><li><a href="https://arxiv.org/abs/1906.05433" target="_blank">Tackling Climate Change with Machine Learning</a></li><li><a href="https://medium.com/sela-labs/an-ethical-awakening-in-ai-a44e1acec17d" target="_blank">An Ethical Awakening in AI</a></li><li><a href="https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/" target="_blank">Training a single AI model can emit as much carbon as five cars in their lifetimes</a></li><li><a href="https://www.media.mit.edu/posts/air-travel-in-academia-designing-a-carbon-offsets-program-for-the-mit-media-lab/" target="_blank">Air travel in academia: Designing a carbon offset program for the MIT Media Lab</a></li><li><a href="https://www.climatechange.ai/ICML2019_workshop.html" target="_blank">https://www.climatechange.ai/ICML2019_workshop.html</a></li><li><a href="https://www.climatechange.ai/NeurIPS2019_workshop.html" target="_blank">https://www.climatechange.ai/NeurIPS2019_workshop.html</a></li></ul>',
					headshot: 'hassan_kane.jpg'
				},
				hong_yu: {
					name: 'Dr. Hong Yu',
					bio: 'Dr. Hong Yu is an elected Fellow of the American College of Medical Informatics. She has joined the Department of Computer Science of the University of Massachusetts at Lowell (UMass-Lowell) as a tenured professor since January of 2018. Before joining UMass-Lowell, she was a tenured full professor in the Department of Quantitative Health Sciences at the University of Massachusetts Medical School. She obtained PhD in Biomedical Informatics from Columbia University. She is an internationally known expert in biomedical and health data science and has been on the Editorial Board of the Journal of Biomedical Informatics since 2009. She has chaired or co-chaired several leading computer science and biomedical informatics conferences. She is the Principle Investigator in multiple NIH and HSR&D grants and has authored over 180 peered-reviewed publications in leading computer science and biomedical informatics journals and conference proceedings.',
					headshot: 'hong_yu.jpg'
				},
				isha_puri: {
					name: 'Isha Puri',
					bio: 'Isha Puri is a freshman at Harvard University passionate about the intersection of computer science, health, and social good. She is the creator of DYSCERN, an application that harnesses eye tracking algorithms to predict risk of dyslexia among children, empowering anybody with access to a laptop to receive a screening. A Coca Cola Scholar and Davidson Fellow, she was recognized as a 2019 ACM Cutler Bell Excellence in High School Computing Laureate, Google Science Fair Global Finalist, and American Academy of Neurology Neuroscience Research Prize Finalist, and 2018 Intel Science Fair Grand Prize Winner and MIT THINK Fellow. Isha is the cofounder of the nonprofit CreAIte, an organization that works to empower young women and underprivileged students to pursue Artificial Intelligence through art. She has spoken about her work at such venues as the MIT Tech Review Conference, International Society for Technology in Education, Columbia University, GIRLCON, and the IEEE Engineering in Medicine and Biology Conference.',
					title: 'A Scalable and Freely Accessible Machine Learning Based Application for the Early Detection of Dyslexia',
					abstract: 'Dyslexia is the world’s most common neurological learning disability - it affects 1 in every 10 people worldwide. And though it has been proven that an early diagnosis can significantly reduce learning difficulties later in life, screenings for dyslexia remain inaccessible to a majority of the world because of their prohibitive cost ($1000 - $2000 for a screening) and their need for specialized scientific equipment. Previous studies have shown that dyslexics exhibit significantly longer and more frequent fixations while reading than non-dyslexic readers. The goal of this research was to build a free web-based application that uses a standard computer webcam to screen a child while reading a passage on the screen. By implementing a novel combination of different machine learning algorithms, this research was able to produce a highly accurate eye tracking method with a maximum possible error of only a few pixels. These eye tracking results were then analyzed to determine the duration and frequency of gaze fixations made while reading. Based on this metric, the application was able to predict if a child has a higher risk of dyslexia with an accuracy of 90.18%, as tested on a dataset of real dyslexic patients with 370 samples classified as high or low risk. Because it is completely free and does not require any hefty scientific equipment, this application provides the first-ever freely available, highly accurate test for risk of dyslexia, that is accessible to millions of families around the world without regard to financial status or physical location.',
					headshot: 'isha_puri.jpg'
				},
				julia_moseyko: {
					name: 'Julia Moseyko',
					bio: 'Julia Moseyko is a second-year undergraduate student at MIT, pursuing a degree in Computer Science and Engineering. She is an executive member of the Machine Intelligence Community at MIT leading the inaugural machine intelligence competition, Generator. Julia is also an undergraduate researcher at MIT CSAIL working on end-to-end reinforcement learning for autonomous vehicles by training in data-driven simulation and then transferring to real-world environments. Over the past two summers, Julia worked on vision-based object detection while interning at Optimus Ride and Udelv, two self-driving vehicle startups. Beforehand and in her spare time, Julia works with Chloe Loughridge on personal projects in machine intelligence spanning education, medical informatics, and financial technology, and together were named Spring 2018 AI Grant Fellows.',
					title: 'Machine Intelligence in Research and Industry',
					abstract: 'This breakout will be a discussion-based session covering some of the key differences between machine intelligence research in academia and industry environments, as well as how progress in each can drive and influence innovation in the other. Our goal is to foster a community-sourced conversation about the interplay of research and product development. Especially as companies across the globe increase efforts to integrate artificial intelligence techniques into their day-to-day operations, we feel it is especially important to be cognizant of the societal and economic motivations that influence what research is done, and how quickly it is adopted into industry. We encourage participants to share anecdotal experiences as well as any aspirations or questions regarding future career paths.',
					headshot: 'julia_moseyko.jpg'
				},
				justin_chen: {
					name: 'Justin Chen',
					bio: 'Justin first learned the words "artificial intelligence" from his aunt when he was seven and has been dreaming of a anime-inspired future ever since. He received both his Bachelor of Arts and Master of Science degrees in computer science from Boston University. During his graduate studies, he extensively researched decentralized asynchronous gradient-based optimization for deep neural networks. When not in class or doing research, he focused intensely on founding the Boston University Machine Intelligence Community. Today, he is a Director, the CEO, the Founder of the Machine Intelligence Community, Inc., which is a non-profit corporation in the process of becoming 501(c)(3), dedicated to democratizing machine intelligence by connecting communities, promoting diversity, providing mentorship, and developing educational material to empower the future leaders in machine intelligence.',
					title: 'Machine Intelligence Community, Inc. A Roadmap to Democratizing AI',
					abstract: 'Machine intelligence is the most important endeavor humans will have ever pursued. Today, most accessible resources occupy a small region of knowledge space in textbooks, classes, YouTube videos, and blog posts. Additionally, most of this knowledge is largely unstructured and difficult to traverse. Structuring, disseminating, and collaborating on a mechanism would ease accessibility, accelerate progress, inform communities as research continues to rapidly expand. Machine Intelligence Community\'s mission is to address this problem by developing a sustainable ecosystem of student communities, conference, and knowledge graph.',
					headshot: 'justin_chen.jpg'
				},
				kate_saenko: {
					name: 'Kate Saenko',
					bio: 'Kate is an Associate Professor of Computer Science at Boston University and director of the <a href="http://ai.bu.edu/" target="_blank">Computer Vision and Learning Group</a>. Her research interests are in the broad area of Artificial Intelligence with a focus on Adaptive Machine Learning, Learning for Vision and Language Understanding, and Deep Learning.',
					headshot: 'kate_saenko.jpg'
				},
				kristian_georgiev: {
					name: 'Kristian Georgiev',
					bio: 'I am a junior at MIT, and currently a co-president of MIT MIC. I am interested in the applications of probability and discrete math in ML. Outside of academics, I enjoy playing volleyball and reading.',
					title: 'Generative Models and Creativity',
					abstract: 'Generative models have enjoyed great success in recent years. There is great effort going towards showing whether such models are able to fully learn the underlying distribution. A higher level question is whether generative models can be truly creative. For that, we will first attempt to define creativity, and then see to what extent are GANs, VAEs and other generative models able of human creativity.',
					headshot: 'kristian_georgiev.jpg'
				},
				lingfen_liu: {
					name: 'Lingfen Liu',
					bio: 'Linfeng Liu is a Ph.D. candidate in the computer science department at Tufts University. His research interest is in Bayesian inference and deep learning. He is recently working on applications in spatial data modeling and network data analysis, through using Gaussian processes and graph neural networks. He also works on image segmentation by combining variational inference and convolutional neural network efficiently.',
					title: 'Localizing and Amortizing: Efficient Inference for Gaussian Process',
					abstract: 'Gaussian process (GP) inference with general likelihood is computationally expensive. To achieve efficient GP inference, recent variational inference methods focus on leveraging inducing points or local information. Inducing-point methods define a variational distribution by a small number of inducing points and then derive the distribution of non-inducing points from these inducing points. These methods save the computation on covariance entries between non-inducing points, but require strong correlations between data points and inducing points. Local inference methods, which only approximate correlations of function values at a local range, are convenient approaches to consider only significant covariance values of the posterior. These methods define a directed acyclic graph over data points and focus mainly on covariance values corresponding to graph edges. However, constructing an optimal graph is difficult: the graph may include some weak correlations while neglecting some strong correlations.<br><br>In this project, we propose an efficient inference method for Gaussian processes: Localized and Amortized Inference based on Nearest neighbors (LAIN). This method uses variational inference and parameterizes the approximate covariance in a sparse pattern such that a data point correlates with only data points in its nearest neighbors (no need to construct an optimal directed acyclic graph). Then we approximately decompose the entire inference task to small homogeneous sub-tasks, each of which is localized to a neighborhood. To reduce variational parameters, we further fit a shared inference network to perform all these sub-tasks. The proposed inference method improves GP performance in data fitting while runs faster than competing methods or at a comparable speed.',
					headshot: 'lingfen_liu.jpg'
				},
				magy_seif_elnasr: {
					name: 'Dr. Magy Seif El-Nasr',
					bio: 'Magy Seif El-Nasr is an associate Professor in the Khoury College of Computer Sciences and the College of Arts, Media and Design, where she directs the Game User Interaction and Intelligence (GUII) Lab. Dr. Seif El-Nasr earned her Ph.D. degree from Northwestern University in Computer Science. Her research focuses on two goals (a) developing automated tools and techniques for authoring, adapting, and personalizing virtual environments (e.g., interactive narrative, believable characters, and visuals), and (b) developing evidence-based methodologies to measure the effectiveness of game environments through the development of novel in-depth behavior mining and visual analytics tools. She recently published the first book on the subject of game analytics, called Game Analytics: Maximizing the Value of Player Data. Her work is internationally known and cited in several game industry books, including <i>Programming Believable Characters for Computer Games (Game Development Series)</i> and <i>Real-time Cinematography for Games</i>. She has received several awards and recognition within the game research community. Notably, she received four <i>Best Paper Awards</i> and several citations in industry books and magazines. She is on the editorial board of: <i>IEEE Transactions on Games and IEEE Transactions on Affective Computing</i>. <a href="http://www.neu.edu/magy" target="_blank">http://www.neu.edu/magy</a>.',
					headshot: 'magy_seif_elnasr.jpg'
				},
				natalia_frumkin: {
					name: 'Natalia Frumkin',
					bio: 'Natalia Frumkin is a senior in computer engineering at Boston University. Her interests lie at the intersection of Machine Learning and High Performance Computing. For the past three years, she has worked in the Visual Information Processing lab on developing a testbed for indoor object detection using light sensors. She is currently working in the Information and Data Sciences lab on developing algorithms for online Divergence learning, a new machine learning framework. She has taken coursework relating to deep learning, high performance computing, and stochastic processes. She has previously worked as an undergraduate teaching fellow for the Introduction to Machine Learning course and an Introduction to Engineering course, where students develop Kinect games using the .NET Core C# framework. This year, she is president of BU High Performance Computing. In her spare time, Natalia loves biking around the city and spending time with friends on the Esplanade.',
					title: 'Online Bregman Divergence Learning',
					abstract: 'Selecting a distance measure is an important task in k-nearest neighbors and clustering algorithms, where the outcomes of a learning algorithm are dependent on how close two points are to one another. The most common distance metric, the Euclidean distance, is useful in cases where the data is well separated in Euclidean space; however, it is often the case in complex tasks that a better distance metric may be found which maximizes the distance between clustered points. The main task of metric learning is to find an optimal distance metric which allows similar points to be grouped together, and dissimilar points to be far apart. We aim to discover an optimal distance from the set of Bregman divergences, a rich class of divergences which includes the euclidean distance, the KL-divergence, and the Itakura-Saito distance. Our current work focuses on online divergence learning where we approximate the Bregman divergence iteratively using a maximum of linear functions. At each timestep, we add an additional linear function, so that the number of parameters grows as we receive more data points.',
					headshot: 'natalia_frumkin.jpg'
				},
				nataniel_ruiz: {
					name: 'Nataniel Ruiz',
					bio: 'Nataniel Ruiz is a second year PhD student at Boston University advised by Prof. Stan Sclaroff. He is interested in computer vision, machine learning, statistics and representation learning. His focus is on facial analysis and data simulation.<br><br>He graduated from Georgia Tech in Fall 2017 with a M.Sc. in Computer Science specializing in Machine Learning, advised by Prof. James Rehg. Previously, he obtained his B.Sc. and M.Sc. in Data Science from Ecole Polytechnique. He completed an internship at Apple AI Research during Summer 2019. In 2018 he was a Spring/Summer intern at the NEC-Labs Media Analytics Department. He also worked as a research intern at MIT CSAIL in 2016.',
					title: 'Learning to Simulate',
					abstract: 'Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.',
					headshot: 'nataniel_ruiz.jpg'
				},
				nishanth_kumar: {
					name: 'Nishanth Kumar',
					bio: 'Nishanth Kumar is an undergraduate student at Brown University who\'s been working as an Undergraduate Researcher for approximately two years. His interests lie at the intersection of AI and Robotics. He has published a few papers at top robotics conferences and has previously spoken at the Ivy League Undergraduate Research Symposium. Outside of research, he enjoys taking advanced classes and is currently the Head Teaching Assistant for a graduate-level reinforcement learning course.<br><br>In his free time, Nishanth likes to read and write fiction (specifically Young Adult Sci-Fi!) and entertain himself with top-quality engineering memes.',
					title: 'Building intelligent, collaborative robots',
					abstract: 'Humans have long dreamed of creating generally-capable robots that can assist us with all the dull, dirty and dangerous tasks that we\'d rather not do. However, despite much progress in a wide variety of fields, we haven\'t yet been able to realize this dream. In this talk, Nishanth will explore the past and present of attempts to create intelligent robots, then talk about some of his own work in the field. Specifically, he\'ll shed some light on how advances in fields like Machine Learning and Augmented Reality can be leveraged for real-world robotic tasks. Finally, he\'ll end with some interesting and exciting directions for future research that might one day help us turn our dream into a reality.',
					headshot: 'nishanth_kumar.jpg'
				},
				rachel_manzelli: {
					name: 'Racehl Manzelli',
					bio: 'Rachel Manzelli is a machine learning software engineer at Marlo, a Boston-based tech startup. She graduated from Boston University in 2019 with a degree in computer engineering. During her time at BU, she was co-president of Boston University Machine Intelligence Community and a mentor for AI4All, a nonprofit working to increase diversity and inclusion in artificial intelligence. She also conducted research in the Information and Data Sciences Lab in the ECE Department as a Lutchen Fellow and 4-time UROP recipient. She focused on solving audio problems with deep learning, including structured audio generation and feature extraction, and has published her work at NeurIPS, ISMIR, and ICCC.',
					title: 'Machine Intelligence in Research and Industry',
					abstract: 'This breakout will be a discussion-based session covering some of the key differences between machine intelligence research in academia and industry environments, as well as how progress in each can drive and influence innovation in the other. Our goal is to foster a community-sourced conversation about the interplay of research and product development. Especially as companies across the globe increase efforts to integrate artificial intelligence techniques into their day-to-day operations, we feel it is especially important to be cognizant of the societal and economic motivations that influence what research is done, and how quickly it is adopted into industry. We encourage participants to share anecdotal experiences as well as any aspirations or questions regarding future career paths.',
					headshot: 'rachel_manzelli.jpg'
				},
				rosanne_liu: {
					name: 'Rosanne Liu',
					bio: '<a href="http://www.rosanneliu.com/" target="_blank">Rosanne</a> is a senior research scientist and a founding member of Uber AI. She obtained her PhD in Computer Science at Northwestern University, where she used neural networks to help discover novel materials. She is currently working on the multiple fronts where machine learning and neural networks are mysterious. She attempts to write in her spare time.',
					headshot: 'rosanne_liu.jpg'
				},
				seungchan_kim: {
					name: 'Seungchan Kim',
					bio: 'Seungchan Kim is a 1st-year master\'s student at Brown University, studying computer science. Previously, he received his bachelor\'s degree from Brown in applied mathematics and computer science. His research focuses on reinforcement learning, and he is interested in other areas within artificial intelligence like computer vision, partially observable MDPs, and robotics. His recent works include the combination of softmax operators and deep reinforcement learning, and investigation of their theoretical properties.',
					title: 'An Alternative Softmax Operator for Deep Reinforcement Learning',
					abstract: 'Reinforcement learning (RL) is a standard framework to study agents that learn to make sequential decisions to maximize long-term reward by interacting with the environment. Recent breakthroughs have shown that the combination of deep learning and reinforcement learning can yield high-level performance in complex domains. I introduce the newly proposed softmax operator, Mellowmax, which can be used in the context of action-selection and value function optimization in RL. I also present the mathematical properties of Mellowmax operator, and show that Mellowmax operator contributes to alleviate the well-known overestimation problem in RL. Lastly, I propose a new DeepMellow algorithm, the combination of Deep RL and Mellowmax, that performs as well as previous deep RL algorithms.',
					headshot: 'seungchan_kim.jpg'
				},
				vasanth_sarathy: {
					name: 'Vasanth Sarathy',
					bio: 'Vasanth Sarathy is a fifth-year dual Ph.D. student in Computer Science and Cognitive Science at Tufts University. His current research is in Natural Language Understanding, focusing on linguistic and conversational phenomena that require commonsense knowledge and reasoning. Phenomena he studies include anaphora, indirect speech acts, and implicatures.  Broadly, his research interests include studying the computational principles behind sensemaking, commonsense reasoning, and creative problem solving, and building artificial agents that can apply these principles ethically.',
					title: 'Natural Language Understanding via Commonsense Reasoning and Sensemaking',
					abstract: 'In this talk, I will discuss two challenging natural language understanding tasks - pronoun resolution and indirect speech act classification. Pronouns and ISAs are pervasive in dialog and discourse, but it has been particularly difficult to interpret them correctly as they are known to require a substantial amount of commonsense knowledge and inference.  Moreover, even slight changes to situational and contextual information can significantly alter their interpretation. Current state-of-the-art statistical approaches and knowledge-based reasoning systems are insufficient to adequately address these tasks. I will introduce a multi-reasoner architecture using Answer Set Programming and uncertainty modeling to disambiguate pronouns and interpret ISAs. In doing so, I will discuss some general types of reasoning and knowledge that might be needed.',
					headshot: 'vasanth_sarathy.jpg'
				},
			}
		}
	}
}
</script>

<style>
</style>